{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-bellam/nlp-lab/blob/main/4th_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import math\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlp8YDZQopxg",
        "outputId": "33c9651e-ac5a-41fa-8530-dee06a4122ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "This is a sample corpus for language modeling.\n",
        "It contains multiple sentences.\n",
        "Language models are used in various NLP applications.\n",
        "We will implement unigram, bigram, and trigram models.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "tINfMhBGpEzh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Tokenize the text\n",
        "tokens = word_tokenize(corpus.lower())\n"
      ],
      "metadata": {
        "id": "kyLMl86oou0I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Generate n-grams\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "unigrams = generate_ngrams(tokens, 1)\n",
        "bigrams = generate_ngrams(tokens, 2)\n",
        "trigrams = generate_ngrams(tokens, 3)\n"
      ],
      "metadata": {
        "id": "EZP-uF2VoyLw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Count n-grams\n",
        "unigram_counts = Counter(unigrams)\n",
        "bigram_counts = Counter(bigrams)\n",
        "trigram_counts = Counter(trigrams)\n"
      ],
      "metadata": {
        "id": "9Eq2Mz_soytw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Compute probabilities with Laplace Smoothing\n",
        "def laplace_smoothing(ngram_counts, total_count, vocab_size, k=1):\n",
        "    smoothed_probs = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        smoothed_probs[ngram] = (count + k) / (total_count + k * vocab_size)\n",
        "    return smoothed_probs\n",
        "\n",
        "vocab_size = len(set(tokens))\n",
        "unigram_probs = laplace_smoothing(unigram_counts, sum(unigram_counts.values()), vocab_size)\n",
        "bigram_probs = laplace_smoothing(bigram_counts, sum(bigram_counts.values()), vocab_size)\n",
        "trigram_probs = laplace_smoothing(trigram_counts, sum(trigram_counts.values()), vocab_size)\n"
      ],
      "metadata": {
        "id": "smDGnLO0o1xw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Compute perplexity\n",
        "def compute_perplexity(test_sentence, ngram_probs, n, vocab_size, k=1):\n",
        "    test_tokens = word_tokenize(test_sentence.lower())\n",
        "    test_ngrams = generate_ngrams(test_tokens, n)\n",
        "    log_prob_sum = 0\n",
        "    num_ngrams = len(test_ngrams)\n",
        "\n",
        "    for ngram in test_ngrams:\n",
        "        prob = ngram_probs.get(ngram, k / (sum(ngram_probs.values()) + k * vocab_size))\n",
        "        log_prob_sum += math.log(prob)\n",
        "\n",
        "    perplexity = math.exp(-log_prob_sum / num_ngrams)\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "Qz5osQ4fo4ff"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example test sentence\n",
        "test_sentence = \"This is a sample\"\n",
        "unigram_perplexity = compute_perplexity(test_sentence, unigram_probs, 1, vocab_size)\n",
        "bigram_perplexity = compute_perplexity(test_sentence, bigram_probs, 2, vocab_size)\n",
        "trigram_perplexity = compute_perplexity(test_sentence, trigram_probs, 3, vocab_size)\n",
        "\n",
        "print(\"Unigram Perplexity:\", unigram_perplexity)\n",
        "print(\"Bigram Perplexity:\", bigram_perplexity)\n",
        "print(\"Trigram Perplexity:\", trigram_perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N7WOXvDpWN_",
        "outputId": "6dba63e4-ed34-4beb-da7d-1afbc6e19bd0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Perplexity: 31.0\n",
            "Bigram Perplexity: 30.499999999999986\n",
            "Trigram Perplexity: 30.000000000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example test sentence\n",
        "test_sentence = \"language models are useful\"\n",
        "unigram_perplexity = compute_perplexity(test_sentence, unigram_probs, 1, vocab_size)\n",
        "bigram_perplexity = compute_perplexity(test_sentence, bigram_probs, 2, vocab_size)\n",
        "trigram_perplexity = compute_perplexity(test_sentence, trigram_probs, 3, vocab_size)\n",
        "\n",
        "print(\"Unigram Perplexity:\", unigram_perplexity)\n",
        "print(\"Bigram Perplexity:\", bigram_perplexity)\n",
        "print(\"Trigram Perplexity:\", trigram_perplexity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KWm5zLWnwqh",
        "outputId": "a4705b40-48d0-4414-a098-e59aac148ceb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Perplexity: 24.892879701299158\n",
            "Bigram Perplexity: 30.01980174164004\n",
            "Trigram Perplexity: 29.5296461204668\n"
          ]
        }
      ]
    }
  ]
}