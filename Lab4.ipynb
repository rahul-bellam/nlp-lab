{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-bellam/nlp-lab/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import math\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return tokens\n",
        "\n",
        "# Function to generate n-grams\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "\n",
        "# Function to train n-gram model\n",
        "def train_ngram_model(tokens, n):\n",
        "    return Counter(generate_ngrams(tokens, n))\n",
        "\n",
        "# Laplace smoothing function\n",
        "def laplace_smoothing(ngram_counts, n_minus1_counts, vocab_size, ngram):\n",
        "    return (ngram_counts[ngram] + 1) / (n_minus1_counts[ngram[:-1]] + vocab_size)\n",
        "\n",
        "# Compute perplexity\n",
        "def compute_perplexity(test_sentence, ngram_counts, n_minus1_counts, vocab_size, n):\n",
        "    test_tokens = preprocess_text(test_sentence)\n",
        "    test_ngrams = generate_ngrams(test_tokens, n)\n",
        "    log_prob_sum = 0\n",
        "    for ngram in test_ngrams:\n",
        "        prob = laplace_smoothing(ngram_counts, n_minus1_counts, vocab_size, ngram)\n",
        "        log_prob_sum += math.log(prob)\n",
        "    perplexity = math.exp(-log_prob_sum / len(test_ngrams))\n",
        "    return perplexity\n",
        "\n",
        "# Sample corpus\n",
        "corpus = \"\"\"\n",
        "    Natural language processing is a subfield of artificial intelligence.\n",
        "    It enables computers to understand human language.\n",
        "    Language models are essential in NLP tasks.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocessing corpus\n",
        "tokens = preprocess_text(corpus)\n",
        "vocab = set(tokens)\n",
        "vocab_size = len(vocab) + 1  # For smoothing\n",
        "\n",
        "# Train models\n",
        "unigram_counts = train_ngram_model(tokens, 1)\n",
        "bigram_counts = train_ngram_model(tokens, 2)\n",
        "trigram_counts = train_ngram_model(tokens, 3)\n",
        "\n",
        "# Compute n-1 gram counts\n",
        "unigram_context_counts = Counter([ug[0] for ug in generate_ngrams(tokens, 1)])\n",
        "bigram_context_counts = Counter([bg[0] for bg in generate_ngrams(tokens, 1)])\n",
        "trigram_context_counts = Counter([tg[:2] for tg in generate_ngrams(tokens, 2)])\n",
        "\n",
        "# User input for test sentence\n",
        "test_sentence = input(\"Enter a sentence: \")\n",
        "print(\"Perplexity for Unigram Model:\", compute_perplexity(test_sentence, unigram_counts, unigram_context_counts, vocab_size, 1))\n",
        "print(\"Perplexity for Bigram Model:\", compute_perplexity(test_sentence, bigram_counts, bigram_context_counts, vocab_size, 2))\n",
        "print(\"Perplexity for Trigram Model:\", compute_perplexity(test_sentence, trigram_counts, trigram_context_counts, vocab_size, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2LPL-O-a5dX",
        "outputId": "33a185ef-8ce0-4c4a-f255-0e59c339ed8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: rahul is a good man\n",
            "Perplexity for Unigram Model: 17.430740514869576\n",
            "Perplexity for Bigram Model: 20.490670517227798\n",
            "Perplexity for Trigram Model: 23.140264701838817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "class LaplaceSmoothing:\n",
        "    def __init__(self, corpus):\n",
        "        self.unigrams = defaultdict(int)\n",
        "        self.bigrams = defaultdict(int)\n",
        "        self.total_unigrams = 0\n",
        "        self.vocab_size = 0\n",
        "\n",
        "        self.train(corpus)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        vocab = set()\n",
        "        for sentence in corpus:\n",
        "            tokens = sentence.split()\n",
        "            vocab.update(tokens)\n",
        "            for i in range(len(tokens)):\n",
        "                self.unigrams[tokens[i]] += 1\n",
        "                self.total_unigrams += 1\n",
        "                if i > 0:\n",
        "                    self.bigrams[(tokens[i-1], tokens[i])] += 1\n",
        "\n",
        "        self.vocab_size = len(vocab)\n",
        "\n",
        "    def bigram_prob(self, word1, word2):\n",
        "        return (self.bigrams[(word1, word2)] + 1) / (self.unigrams[word1] + self.vocab_size)\n",
        "\n",
        "corpus = [\"the cat sat on the mat\", \"the dog sat on the mat\"]\n",
        "model = LaplaceSmoothing(corpus)\n",
        "\n",
        "print(f\"P(cat | the): {model.bigram_prob('the', 'cat'):.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzrF8g5Sa5aA",
        "outputId": "43a8b08c-0f81-49be-b046-d94c0e9fa9b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(cat | the): 0.200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "class NGramModel:\n",
        "    def __init__(self, n, smoothing=0.001): # Changed _init_ to __init__\n",
        "        \"\"\"\n",
        "        Initialize the N-Gram model.\n",
        "        :param n: Order of the n-gram (1 for unigram, 2 for bigram, etc.)\n",
        "        :param smoothing: Smoothing factor (Lidstone Smoothing)\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.model = defaultdict(lambda: 0)  # Stores n-gram counts\n",
        "        self.context_counts = defaultdict(lambda: 0)  # Stores (N-1)-gram counts\n",
        "        self.smoothing = smoothing  # Lidstone smoothing factor\n",
        "        self.vocab_size = 0  # Vocabulary size\n",
        "\n",
        "    def tokenize_and_pad(self, corpus):\n",
        "        \"\"\"\n",
        "        Tokenizes and adds padding to the corpus.\n",
        "        :param corpus: Input text corpus\n",
        "        :return: Tokenized and padded n-grams list\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(corpus.lower())  # Convert text to lowercase and tokenize\n",
        "        self.vocab_size = len(set(tokens))  # Count unique words (Vocabulary size)\n",
        "\n",
        "        # Generate n-grams with padding\n",
        "        ngrams_list = list(ngrams(tokens, self.n, pad_left=True, pad_right=True,\n",
        "                                  left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "        return ngrams_list\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\"\n",
        "        Train the N-Gram model on a given text corpus.\n",
        "        :param corpus: Input text corpus\n",
        "        \"\"\"\n",
        "        ngrams_list = self.tokenize_and_pad(corpus)  # Tokenize and pad the corpus\n",
        "\n",
        "        # Count N-grams\n",
        "        fdist = FreqDist(ngrams_list)\n",
        "        self.model = fdist  # Store n-gram counts\n",
        "\n",
        "        # Count (N-1)-grams for probability calculation\n",
        "        for ngram in ngrams_list:\n",
        "            context = ngram[:-1]  # Extract (N-1) prefix\n",
        "            self.context_counts[context] += 1  # Count occurrences\n",
        "\n",
        "    def raw_probability(self, ngram):\n",
        "        \"\"\"\n",
        "        Compute raw probability without smoothing.\n",
        "        :param ngram: The n-gram tuple\n",
        "        :return: Raw probability\n",
        "        \"\"\"\n",
        "        count_ngram = self.model[ngram]  # Count of the full N-gram\n",
        "        context = ngram[:-1]  # Extract (N-1)-gram\n",
        "        count_context = self.context_counts[context]  # Count of (N-1)-gram\n",
        "\n",
        "        # Avoid division by zero\n",
        "        if count_context == 0:\n",
        "            return 0\n",
        "        return count_ngram / count_context\n",
        "\n",
        "    def smoothed_probability(self, ngram):\n",
        "        \"\"\"\n",
        "        Compute probability with Lidstone smoothing.\n",
        "        :param ngram: The n-gram tuple\n",
        "        :return: Smoothed probability\n",
        "        \"\"\"\n",
        "        count_ngram = self.model[ngram]  # Count of the full N-gram\n",
        "        context = ngram[:-1]  # Extract (N-1)-gram\n",
        "        count_context = self.context_counts[context]  # Count of (N-1)-gram\n",
        "\n",
        "        # Apply Lidstone Smoothing\n",
        "        smoothed_prob = (count_ngram + self.smoothing) / (count_context + self.vocab_size * self.smoothing)\n",
        "        return smoothed_prob\n",
        "\n",
        "    def calculate_perplexity(self, test_sentence):\n",
        "        \"\"\"\n",
        "        Compute perplexity of a test sentence.\n",
        "        :param test_sentence: Input test sentence\n",
        "        :return: Perplexity value\n",
        "        \"\"\"\n",
        "        ngrams_list = self.tokenize_and_pad(test_sentence)  # Tokenize and pad test sentence\n",
        "\n",
        "        log_prob_sum = 0\n",
        "        for ngram in ngrams_list:\n",
        "            prob_before = self.raw_probability(ngram)  # Raw probability (before smoothing)\n",
        "            prob_after = self.smoothed_probability(ngram)  # Smoothed probability\n",
        "\n",
        "            print(f\"N-gram: {ngram}, Raw Prob: {prob_before}, Smoothed Prob: {prob_after}\")  # Debug Info\n",
        "\n",
        "            log_prob_sum += math.log2(prob_after)  # Sum log probabilities\n",
        "\n",
        "        N = len(ngrams_list)  # Total number of n-grams in test sentence\n",
        "        perplexity = 2 ** (-log_prob_sum / N)  # Compute perplexity\n",
        "        return perplexity\n",
        "\n",
        "# Example usage\n",
        "corpus = \"This is a test sentence. This is another test sentence.\"\n",
        "test_sentence = \"This is a test.\"\n",
        "\n",
        "# Unigram Model\n",
        "print(\"\\n---- Unigram Model ----\")\n",
        "unigram_model = NGramModel(n=1)\n",
        "unigram_model.train(corpus)\n",
        "print(\"Unigram Perplexity:\", unigram_model.calculate_perplexity(test_sentence))\n",
        "\n",
        "# Bigram Model\n",
        "print(\"\\n---- Bigram Model ----\")\n",
        "bigram_model = NGramModel(n=2)\n",
        "bigram_model.train(corpus)\n",
        "print(\"Bigram Perplexity:\", bigram_model.calculate_perplexity(test_sentence))\n",
        "\n",
        "# Trigram Model\n",
        "print(\"\\n---- Trigram Model ----\")\n",
        "trigram_model = NGramModel(n=3)\n",
        "trigram_model.train(corpus)\n",
        "print(\"Trigram Perplexity:\", trigram_model.calculate_perplexity(test_sentence))"
      ],
      "metadata": {
        "id": "5MkzeOopbTwQ",
        "outputId": "73793f00-cb7a-45d4-ac51-08650efd3b11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Unigram Model ----\n",
            "N-gram: ('this',), Raw Prob: 0.16666666666666666, Smoothed Prob: 0.16668054977092875\n",
            "N-gram: ('is',), Raw Prob: 0.16666666666666666, Smoothed Prob: 0.16668054977092875\n",
            "N-gram: ('a',), Raw Prob: 0.08333333333333333, Smoothed Prob: 0.08338192419825072\n",
            "N-gram: ('test',), Raw Prob: 0.16666666666666666, Smoothed Prob: 0.16668054977092875\n",
            "N-gram: ('.',), Raw Prob: 0.16666666666666666, Smoothed Prob: 0.16668054977092875\n",
            "Unigram Perplexity: 6.890927457103827\n",
            "\n",
            "---- Bigram Model ----\n",
            "N-gram: ('<s>', 'this'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('this', 'is'), Raw Prob: 1.0, Smoothed Prob: 0.9980049875311721\n",
            "N-gram: ('is', 'a'), Raw Prob: 0.5, Smoothed Prob: 0.4992518703241895\n",
            "N-gram: ('a', 'test'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('test', '.'), Raw Prob: 0.0, Smoothed Prob: 0.0004987531172069826\n",
            "N-gram: ('.', '</s>'), Raw Prob: 0.5, Smoothed Prob: 0.4992518703241895\n",
            "Bigram Perplexity: 4.4836775498499035\n",
            "\n",
            "---- Trigram Model ----\n",
            "N-gram: ('<s>', '<s>', 'this'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('<s>', 'this', 'is'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('this', 'is', 'a'), Raw Prob: 0.5, Smoothed Prob: 0.4992518703241895\n",
            "N-gram: ('is', 'a', 'test'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "N-gram: ('a', 'test', '.'), Raw Prob: 0.0, Smoothed Prob: 0.0009950248756218907\n",
            "N-gram: ('test', '.', '</s>'), Raw Prob: 0, Smoothed Prob: 0.2\n",
            "N-gram: ('.', '</s>', '</s>'), Raw Prob: 1.0, Smoothed Prob: 0.9960199004975124\n",
            "Trigram Perplexity: 3.7395609380622976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}